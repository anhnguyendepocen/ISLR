Springer Texts in Statistics
Series Editors:
G. Casella S. Fienberg I. Olkin
For further volumes: http://www.springer.com/series/417
Gareth James • Daniela Witten • Trevor Hastie Robert Tibshirani
An Introduction to Statistical Learning
with Applications in R
123
Gareth James
Department of Information and
Operations Management University of Southern California Los Angeles, CA, USA
Trevor Hastie Department of Statistics Stanford University Stanford, CA, USA
Daniela Witten Department of Biostatistics University of Washington Seattle, WA, USA
Robert Tibshirani Department of Statistics Stanford University Stanford, CA, USA
ISSN 1431-875X
ISBN 978-1-4614-7137-0
DOI 10.1007/978-1-4614-7138-7
Springer New York Heidelberg Dordrecht London
Library of Congress Control Number: 2013936251
© Springer Science+Business Media New York 2013 (Corrected at 4 printing 2014)
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissim- ilar methodology now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied specifically for the pur- pose of being entered and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher’s location, in its current version, and permission for use must always be obtained from Springer. Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publi- cation does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)
ISBN 978-1-4614-7138-7 (eBook)
To our parents:
Alison and Michael James Chiara Nappi and Edward Witten Valerie and Patrick Hastie Vera and Sami Tibshirani
and to our families:
Michael, Daniel, and Catherine Ari
Samantha, Timothy, and Lynda Charlie, Ryan, Julie, and Cheryl
Preface
Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. The field encompasses many methods such as the lasso and sparse regression, classification and regression trees, and boosting and support vector machines.
With the explosion of “Big Data” problems, statistical learning has be- come a very hot field in many scientific areas as well as marketing, finance, and other business disciplines. People with statistical learning skills are in high demand.
One of the first books in this area—The Elements of Statistical Learning (ESL) (Hastie, Tibshirani, and Friedman)—was published in 2001, with a second edition in 2009. ESL has become a popular text not only in statis- tics but also in related fields. One of the reasons for ESL’s popularity is its relatively accessible style. But ESL is intended for individuals with ad- vanced training in the mathematical sciences. An Introduction to Statistical Learning (ISL) arose from the perceived need for a broader and less tech- nical treatment of these topics. In this new book, we cover many of the same topics as ESL, but we concentrate more on the applications of the methods and less on the mathematical details. We have created labs illus- trating how to implement each of the statistical learning methods using the popular statistical software package R. These labs provide the reader with valuable hands-on experience.
This book is appropriate for advanced undergraduates or master’s stu- dents in statistics or related quantitative fields or for individuals in other
vii
viii Preface
disciplines who wish to use statistical learning tools to analyze their data. It can be used as a textbook for a course spanning one or two semesters.
We would like to thank several readers for valuable comments on prelim- inary drafts of this book: Pallavi Basu, Alexandra Chouldechova, Patrick Danaher, Will Fithian, Luella Fu, Sam Gross, Max Grazier G’Sell, Court- ney Paulson, Xinghao Qiao, Elisa Sheng, Noah Simon, Kean Ming Tan, and Xin Lu Tan.
It’s tough to make predictions, especially about the future.
Los Angeles, USA Seattle, USA Palo Alto, USA Palo Alto, USA
-Yogi Berra
Gareth James Daniela Witten Trevor Hastie Robert Tibshirani
Contents
Preface vii
1 Introduction 1
2 Statistical Learning 15
2.1 WhatIsStatisticalLearning?................. 15
2.1.1 WhyEstimatef?.................... 17
2.1.2 HowDoWeEstimatef? ............... 21
2.1.3 The Trade-Off Between Prediction Accuracy
andModelInterpretability .............. 24
2.1.4 Supervised Versus Unsupervised Learning . . . . . . 26
2.1.5 Regression Versus Classification Problems . . . . . . 28
2.2 AssessingModelAccuracy................... 29 2.2.1 MeasuringtheQualityofFit ............. 29 2.2.2 TheBias-VarianceTrade-Off ............. 33 2.2.3 TheClassificationSetting ............... 37
2.3 Lab:IntroductiontoR..................... 42
2.3.1 BasicCommands.................... 42
2.3.2 Graphics ........................ 45
2.3.3 IndexingData ..................... 47
2.3.4 LoadingData...................... 48
2.3.5 Additional Graphical and Numerical Summaries . . 49
2.4 Exercises ............................ 52
ix
x
Contents
3
Linear Regression 59
3.1 SimpleLinearRegression ................... 61
3.1.1 EstimatingtheCoefficients .............. 61
3.1.2 Assessing the Accuracy of the Coefficient
Estimates........................ 63
3.1.3 AssessingtheAccuracyoftheModel . . . . . . . . . 68
3.2 MultipleLinearRegression .................. 71
3.2.1 Estimating the Regression Coefficients . . . . . . . . 72 3.2.2 SomeImportantQuestions .............. 75
3.3 Other Considerations in the Regression Model . . . . . . . . 82
3.3.1 QualitativePredictors ................. 82
3.3.2 ExtensionsoftheLinearModel . . . . . . . . . . . . 86
3.3.3 PotentialProblems................... 92
3.4 TheMarketingPlan ...................... 102
3.5 Comparison of Linear Regression with K -Nearest Neighbors............................ 104
3.6 Lab:LinearRegression..................... 109 3.6.1 Libraries......................... 109 3.6.2 SimpleLinearRegression ............... 110 3.6.3 MultipleLinearRegression .............. 113 3.6.4 InteractionTerms ................... 115 3.6.5 Non-linear Transformations of the Predictors . . . . 115 3.6.6 QualitativePredictors ................. 117 3.6.7 WritingFunctions ................... 119
3.7 Exercises ............................ 120
4 Classification 127
4.1 AnOverviewofClassification................. 128
4.2 WhyNotLinearRegression? ................. 129
4.3 LogisticRegression....................... 130
4.3.1 TheLogisticModel................... 131 4.3.2 Estimating the Regression Coefficients . . . . . . . . 133 4.3.3 MakingPredictions................... 134 4.3.4 MultipleLogisticRegression. . . . . . . . . . . . . . 135 4.3.5 Logistic Regression for >2 Response Classes . . . . . 137
4.4 LinearDiscriminantAnalysis ................. 138 4.4.1 Using Bayes’ Theorem for Classification . . . . . . . 138 4.4.2 LinearDiscriminantAnalysisforp=1. . . . . . . . 139 4.4.3 Linear Discriminant Analysis for p >1 . . . . . . . . 142 4.4.4 Quadratic Discriminant Analysis . . . . . . . . . . . 149
4.5 AComparisonofClassificationMethods . . . . . . . . . . . 151
4.6 Lab: Logistic Regression, LDA, QDA, and KNN . . . . . . 154 4.6.1 TheStockMarketData ................ 154 4.6.2 LogisticRegression................... 156 4.6.3 LinearDiscriminantAnalysis . . . . . . . . . . . . . 161
4.6.4 Quadratic Discriminant Analysis . . . . . . . . . . . 163 4.6.5 K-NearestNeighbors.................. 163 4.6.6 An Application to Caravan Insurance Data . . . . . 165
4.7 Exercises ............................ 168
5 Resampling Methods 175
5.1 Cross-Validation ........................ 176
5.1.1 TheValidationSetApproach ............. 176
5.1.2 Leave-One-Out Cross-Validation . . . . . . . . . . . 178
5.1.3 k-FoldCross-Validation ................ 181
5.1.4 Bias-Variance Trade-Off for k-Fold
Cross-Validation .................... 183
5.1.5 Cross-Validation on Classification Problems . . . . . 184
5.2 TheBootstrap ......................... 187
5.3 Lab: Cross-Validation and the Bootstrap . . . . . . . . . . . 190 5.3.1 TheValidationSetApproach ............. 191 5.3.2 Leave-One-Out Cross-Validation . . . . . . . . . . . 192 5.3.3 k-FoldCross-Validation ................ 193 5.3.4 TheBootstrap ..................... 194
5.4 Exercises ............................ 197
6 Linear Model Selection and Regularization 203
6.1 SubsetSelection ........................ 205 6.1.1 BestSubsetSelection ................. 205 6.1.2 StepwiseSelection ................... 207 6.1.3 ChoosingtheOptimalModel ............. 210
6.2 ShrinkageMethods....................... 214 6.2.1 RidgeRegression.................... 215 6.2.2 TheLasso........................ 219 6.2.3 SelectingtheTuningParameter. . . . . . . . . . . . 227
6.3 DimensionReductionMethods ................ 228 6.3.1 Principal Components Regression . . . . . . . . . . . 230 6.3.2 PartialLeastSquares ................. 237
6.4 ConsiderationsinHighDimensions . . . . . . . . . . . . . . 238 6.4.1 High-DimensionalData ................ 238 6.4.2 What Goes Wrong in High Dimensions? . . . . . . . 239 6.4.3 RegressioninHighDimensions . . . . . . . . . . . . 241 6.4.4 Interpreting Results in High Dimensions . . . . . . . 243
6.5 Lab1:SubsetSelectionMethods ............... 244
6.5.1 BestSubsetSelection ................. 244
6.5.2 Forward and Backward Stepwise Selection . . . . . . 247
6.5.3 Choosing Among Models Using the Validation
Set Approach and Cross-Validation . . . . . . . . . . 248
Contents xi
xii
Contents
7
6.6 Lab2:RidgeRegressionandtheLasso. . . . . . . . . . . . 251 6.6.1 RidgeRegression.................... 251 6.6.2 TheLasso........................ 255
6.7 Lab3:PCRandPLSRegression ............... 256 6.7.1 Principal Components Regression . . . . . . . . . . . 256 6.7.2 PartialLeastSquares ................. 258
6.8 Exercises ............................ 259
Moving Beyond Linearity 265
7.1 PolynomialRegression..................... 266
7.2 StepFunctions ......................... 268
7.3 BasisFunctions......................... 270
7.4 RegressionSplines ....................... 271
7.4.1 PiecewisePolynomials................. 271
7.4.2 ConstraintsandSplines ................ 271
7.4.3 TheSplineBasisRepresentation . . . . . . . . . . . 273
7.4.4 Choosing the Number and Locations
oftheKnots ...................... 274
7.4.5 Comparison to Polynomial Regression . . . . . . . . 276
7.5 SmoothingSplines ....................... 277 7.5.1 AnOverviewofSmoothingSplines . . . . . . . . . . 277 7.5.2 Choosing the Smoothing Parameter λ . . . . . . . . 278
7.6 LocalRegression ........................ 280
7.7 GeneralizedAdditiveModels ................. 282 7.7.1 GAMsforRegressionProblems . . . . . . . . . . . . 283 7.7.2 GAMs for Classification Problems . . . . . . . . . . 286
7.8 Lab:Non-linearModeling ................... 287 7.8.1 Polynomial Regression and Step Functions . . . . . 288 7.8.2 Splines.......................... 293 7.8.3 GAMs.......................... 294
7.9 Exercises ............................ 297
Tree-Based Methods 303
8.1 TheBasicsofDecisionTrees ................. 303 8.1.1 RegressionTrees .................... 304 8.1.2 ClassificationTrees................... 311 8.1.3 TreesVersusLinearModels .............. 314 8.1.4 Advantages and Disadvantages of Trees . . . . . . . 315
8.2 Bagging,RandomForests,Boosting . . . . . . . . . . . . . 316 8.2.1 Bagging......................... 316 8.2.2 RandomForests .................... 320 8.2.3 Boosting......................... 321
8.3 Lab:DecisionTrees....................... 324 8.3.1 FittingClassificationTrees .............. 324 8.3.2 FittingRegressionTrees................ 327
8
8.3.3 BaggingandRandomForests............. 328
8.3.4 Boosting......................... 330 8.4 Exercises ............................ 332
9 Support Vector Machines 337
9.1 MaximalMarginClassifier................... 338 9.1.1 WhatIsaHyperplane? ................ 338 9.1.2 Classification Using a Separating Hyperplane . . . . 339 9.1.3 TheMaximalMarginClassifier . . . . . . . . . . . . 341 9.1.4 Construction of the Maximal Margin Classifier . . . 342 9.1.5 TheNon-separableCase................ 343
9.2 SupportVectorClassifiers................... 344 9.2.1 Overview of the Support Vector Classifier . . . . . . 344 9.2.2 Details of the Support Vector Classifier . . . . . . . 345
9.3 SupportVectorMachines ................... 349
9.3.1 Classification with Non-linear Decision
Boundaries ....................... 349 9.3.2 TheSupportVectorMachine ............. 350 9.3.3 An Application to the Heart Disease Data . . . . . . 354
9.4 SVMswithMorethanTwoClasses. . . . . . . . . . . . . . 355 9.4.1 One-Versus-OneClassification. . . . . . . . . . . . . 355 9.4.2 One-Versus-AllClassification . . . . . . . . . . . . . 356
9.5 RelationshiptoLogisticRegression . . . . . . . . . . . . . . 356
9.6 Lab:SupportVectorMachines ................ 359 9.6.1 SupportVectorClassifier ............... 359 9.6.2 SupportVectorMachine................ 363 9.6.3 ROCCurves ...................... 365 9.6.4 SVMwithMultipleClasses .............. 366 9.6.5 Application to Gene Expression Data . . . . . . . . 366
9.7 Exercises ............................ 368
10 Unsupervised Learning 373
10.1 The Challenge of Unsupervised Learning . . . . . . . . . . . 373 10.2PrincipalComponentsAnalysis................ 374 10.2.1 What Are Principal Components? . . . . . . . . . . 375 10.2.2 Another Interpretation of Principal Components . . 379 10.2.3 MoreonPCA...................... 380 10.2.4 Other Uses for Principal Components . . . . . . . . 385 10.3ClusteringMethods....................... 385 10.3.1 K-MeansClustering .................. 386 10.3.2 HierarchicalClustering................. 390 10.3.3 PracticalIssuesinClustering . . . . . . . . . . . . . 399 10.4 Lab1:PrincipalComponentsAnalysis . . . . . . . . . . . . 401
Contents xiii
xiv
Contents
10.5Lab2:Clustering........................ 404 10.5.1 K-MeansClustering .................. 404 10.5.2 HierarchicalClustering................. 406
10.6Lab3:NCI60DataExample ................. 407 10.6.1 PCAontheNCI60Data ............... 408 10.6.2 Clustering the Observations of the NCI60 Data . . . 410
10.7Exercises ............................ 413
Index 419