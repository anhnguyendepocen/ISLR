8
Tree-Based Methods
In this chapter, we describe tree-based methods for regression and classification. These involve stratifying or segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observa- tions in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods.
Tree-based methods are simple and useful for interpretation. However, they typically are not competitive with the best supervised learning ap- proaches, such as those seen in Chapters 6 and 7, in terms of prediction accuracy. Hence in this chapter we also introduce bagging, random forests, and boosting. Each of these approaches involves producing multiple trees which are then combined to yield a single consensus prediction. We will see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in inter- pretation.
8.1 The Basics of Decision Trees
Decision trees can be applied to both regression and classification problems. We first consider regression problems, and then move on to classification.
G. James et al., An Introduction to Statistical Learning: with Applications in R, 303 Springer Texts in Statistics, DOI 10.1007/978-1-4614-7138-7 8,
© Springer Science+Business Media New York 2013
decision tree
￼
304 8. Tree-Based Methods
Years| < 4.5
￼Hits < 117.5
￼5.11
6.00
6.74
FIGURE 8.1. For the Hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year. At a given internal node, the label (of the form Xj < tk) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to Xj ≥ tk. For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to Years<4.5, and the right-hand branch corresponds to Years>=4.5. The tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there.
8.1.1 Regression Trees
In order to motivate regression trees, we begin with a simple example. Predicting Baseball Players’ Salaries Using Regression Trees
We use the Hitters data set to predict a baseball player’s Salary based on Years (the number of years that he has played in the major leagues) and Hits (the number of hits that he made in the previous year). We first remove observations that are missing Salary values, and log-transform Salary so that its distribution has more of a typical bell-shape. (Recall that Salary is measured in thousands of dollars.)
Figure 8.1 shows a regression tree fit to this data. It consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having Years<4.5 to the left branch.1 The predicted salary
1Both Years and Hits are integers in these data; the tree() function in R labels the splits at the midpoint between two adjacent values.
regression tree
￼
1 4.5
24
238
117.5
1
Years
8.1 The Basics of Decision Trees 305
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼R1
R3
R2
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼FIGURE 8.2. The three-region partition for the Hitters data set from the regression tree illustrated in Figure 8.1.
for these players is given by the mean response value for the players in the data set with Years<4.5. For such players, the mean log salary is 5.107, and so we make a prediction of e5.107 thousands of dollars, i.e. $165,174, for these players. Players with Years>=4.5 are assigned to the right branch, and then that group is further subdivided by Hits. Overall, the tree stratifies or segments the players into three regions of predictor space: players who have played for four or fewer years, players who have played for five or more years and who made fewer than 118 hits last year, and players who have played for five or more years and who made at least 118 hits last year. These three regions can be written as R1 ={X | Years<4.5}, R2 ={X | Years>=4.5, Hits<117.5}, and R3 ={X | Years>=4.5, Hits>=117.5}. Figure 8.2 illustrates the regions as a function of Years and Hits. The predicted salaries for these three groups are $1,000×e5.107 =$165,174, $1,000×e5.999 =$402,834, and $1,000×e6.740 =$845,346 respectively.
In keeping with the tree analogy, the regions R1, R2, and R3 are known as terminal nodes or leaves of the tree. As is the case for Figure 8.1, decision trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree. The points along the tree where the predictor space is split are referred to as internal nodes. In Figure 8.1, the two internal nodes are indicated by the text Years<4.5 and Hits<117.5. We refer to the segments of the trees that connect the nodes as branches.
We might interpret the regression tree displayed in Figure 8.1 as follows: Years is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who
terminal node leaf
internal node branch
Hits
306 8. Tree-Based Methods
have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries. The regression tree shown in Figure 8.1 is likely an over-simplification of the true relationship between Hits, Years, and Salary. However, it has advantages over other types of regression models (such as those seen in Chapters 3 and 6): it is easier to interpret, and has a nice graphical representation.
Prediction via Stratification of the Feature Space
We now discuss the process of building a regression tree. Roughly speaking, there are two steps.
1. We divide the predictor space—that is, the set of possible values for X1, X2, . . . , Xp—into J distinct and non-overlapping regions, R1,R2,...,RJ.
2. For every observation that falls into the region Rj , we make the same prediction, which is simply the mean of the response values for the training observations in Rj.
For instance, suppose that in Step 1 we obtain two regions, R1 and R2, and that the response mean of the training observations in the first region is 10, while the response mean of the training observations in the second region is 20. Then for a given observation X = x, if x ∈ R1 we will predict avalueof10,andifx∈R2 wewillpredictavalueof20.
We now elaborate on Step 1 above. How do we construct the regions R1,...,RJ? In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predic- tive model. The goal is to find boxes R1, . . . , RJ that minimize the RSS, given by
􏰏J 􏰏 ( y i − y ˆ R j ) 2 , ( 8 . 1 ) j=1 i∈Rj
whereyˆRj isthemeanresponseforthetrainingobservationswithinthe jth box. Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into J boxes. For this reason, we take atop-down,greedyapproachthatisknownasrecursivebinarysplitting.The approachistop-downbecauseitbeginsatthetopofthetree(atwhichpoint all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.
recursive binary splitting
8.1 The Basics of Decision Trees 307
In order to perform recursive binary splitting, we first select the pre- dictor Xj and the cutpoint s such that splitting the predictor space into the regions {X|Xj < s} and {X|Xj ≥ s} leads to the greatest possible reduction in RSS. (The notation {X|Xj < s} means the region of predictor space in which Xj takes on a value less than s.) That is, we consider all predictors X1, . . . , Xp, and all possible values of the cutpoint s for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS. In greater detail, for any j and s, we define the pair of half-planes
R1(j, s) = {X|Xj < s} and R2(j, s) = {X|Xj ≥ s}, (8.2) and we seek the value of j and s that minimize the equation
􏰏 (yi −yˆR1)2 + 􏰏 (yi −yˆR2)2, (8.3) i: xi∈R1(j,s) i: xi∈R2(j,s)
where yˆR1 is the mean response for the training observations in R1(j,s), and yˆR2 is the mean response for the training observations in R2(j,s). Finding the values of j and s that minimize (8.3) can be done quite quickly, especially when the number of features p is not too large.
Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.
OncetheregionsR1,...,RJ havebeencreated,wepredicttheresponse for a given test observation using the mean of the training observations in the region to which that test observation belongs.
A five-region example of this approach is shown in Figure 8.3.
Tree Pruning
The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits (that is, fewer regions R1,...,RJ) might lead to lower variance and better interpretation at the cost of a little bias. One possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold. This strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.
￼308
8. Tree-Based Methods
X2
X2
X2 ≤ t2
X1 ≤ t1 |
X1
X1 ≤ t3
X2 ≤ t4
R1 R2 R3
R4
R5
FIGURE 8.3. Top Left: A partition of two-dimensional feature space that could not result from recursive binary splitting. Top Right: The output of recursive binary splitting on a two-dimensional example. Bottom Left: A tree corresponding to the partition in the top right panel. Bottom Right: A perspective plot of the prediction surface corresponding to that tree.
Therefore, a better strategy is to grow a very large tree T0, and then prune it back in order to obtain a subtree. How do we determine the best way to prune the tree? Intuitively, our goal is to select a subtree that leads to the lowest test error rate. Given a subtree, we can estimate its test error using cross-validation or the validation set approach. However, estimating the cross-validation error for every possible subtree would be too cumbersome, since there is an extremely large number of possible subtrees. Instead, we need a way to select a small set of subtrees for consideration.
Cost complexity pruning—also known as weakest link pruning—gives us a way to do just this. Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter α.
prune subtree
cost complexity pruning
weakest link pruning
t2
R3
X1
X2
X1
R2
R1
R5
R4
t4
t1
t3
8.1 The Basics of Decision Trees 309
￼Algorithm 8.1 Building a Regression Tree
1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.
2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of α.
3. Use K-fold cross-validation to choose α. That is, divide the training observations into K folds. For each k = 1, . . . , K:
(a) Repeat Steps 1 and 2 on all but the kth fold of the training data. (b) Evaluate the mean squared prediction error on the data in the
left-out kth fold, as a function of α.
Average the results for each value of α, and pick α to minimize the
average error.
4. Return the subtree from Step 2 that corresponds to the chosen value
of α.
For each value of α there corresponds a subtree T ⊂ T0 such that
|T|
􏰏 􏰏 (yi−yˆRm)2+α|T| (8.4)
m=1 i: xi∈Rm
is as small as possible. Here |T| indicates the number of terminal nodes of the tree T , Rm is the rectangle (i.e. the subset of predictor space) cor- respondingtothemthterminalnode,andyˆRm isthepredictedresponse associated with Rm—that is, the mean of the training observations in Rm. The tuning parameter α controls a trade-off between the subtree’s com- plexity and its fit to the training data. When α = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error. However, as α increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree. Equation 8.4 is reminiscent of the lasso (6.7) from Chapter 6, in which a similar formulation was used in order to control the complexity of a linear model.
It turns out that as we increase α from zero in (8.4), branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of subtrees as a function of α is easy. We can select a value of α using a validation set or using cross-validation. We then return to the full data set and obtain the subtree corresponding to α. This process is summarized in Algorithm 8.1.
￼￼
310 8. Tree-Based Methods
RBI < 60.5
Putouts < 82
Years < 3.5
Hits < 117.5
Years|< 4.5
￼￼￼￼5.487
4.622
Years < 3.5 5.394 6.189
5.183
Walks < 52.5 Runs < 47.5
￼￼6.015 5.571
6.407 6.549
RBI < 80.5 Years < 6.5
6.459 7.007
FIGURE 8.4. Regression tree analysis for the Hitters data. The unpruned tree that results from top-down greedy splitting on the training data is shown.
Figures 8.4 and 8.5 display the results of fitting and pruning a regression tree on the Hitters data, using nine of the features. First, we randomly divided the data set in half, yielding 132 observations in the training set and 131 observations in the test set. We then built a large regression tree on the training data and varied α in (8.4) in order to create subtrees with different numbers of terminal nodes. Finally, we performed six-fold cross- validation in order to estimate the cross-validated MSE of the trees as a function of α. (We chose to perform six-fold cross-validation because 132 is an exact multiple of six.) The unpruned regression tree is shown in Figure 8.4. The green curve in Figure 8.5 shows the CV error as a function of the number of leaves,2 while the orange curve indicates the test error. Also shown are standard error bars around the estimated errors. For reference, the training error curve is shown in black. The CV error is a reasonable approximation of the test error: the CV error takes on its
2Although CV error is computed as a function of α, it is convenient to display the result as a function of |T |, the number of leaves; this is based on the relationship between α and |T | in the original tree grown to all the training data.
< 43.5 Walks
7.289
￼
8.1 The Basics of Decision Trees 311
￼￼Training Cross−Validation Test
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼2 4 6 8 10
Tree Size
FIGURE 8.5. Regression tree analysis for the Hitters data. The training, cross-validation, and test MSE are shown as a function of the number of termi- nal nodes in the pruned tree. Standard error bands are displayed. The minimum cross-validation error occurs at a tree size of three.
minimum for a three-node tree, while the test error also dips down at the three-node tree (though it takes on its lowest value at the ten-node tree). The pruned tree containing three terminal nodes is shown in Figure 8.1.
8.1.2 Classification Trees
A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. Re- call that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. In interpreting the results of a classification tree, we are often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region.
classification tree
The task of growing a classification tree is quite similar to the task of
growing a regression tree. Just as in the regression setting, we use recursive
binary splitting to grow a classification tree. However, in the classification
setting, RSS cannot be used as a criterion for making the binary splits.
A natural alternative to RSS is the classification error rate. Since we plan
to assign an observation in a given region to the most commonly occurring error rate class of training observations in that region, the classification error rate is
simply the fraction of the training observations in that region that do not belong to the most common class:
classification
Mean Squared Error
0.0 0.2 0.4 0.6 0.8 1.0
312 8. Tree-Based Methods
E = 1 − m a x ( pˆ m k ) . ( 8 . 5 ) k
Here pˆmk represents the proportion of training observations in the mth region that are from the kth class. However, it turns out that classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.
The Gini index is defined by
􏰏K k=1
Gini index
G =
pˆ m k ( 1 − pˆ m k ) ,
( 8 . 6 )
a measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the pˆmk’s are close to zero or one. For this reason the Gini index is referred to as a measure of node purity—a small value indicates that a node contains predominantly observations from a single class.
An alternative to the Gini index is cross-entropy, given by
􏰏K k=1
cross- entropy
D = −
pˆmk log pˆmk.
(8.7)
Since 0 ≤ pˆmk ≤ 1, it follows that 0 ≤ −pˆmk logpˆmk. One can show that the cross-entropy will take on a value near zero if the pˆmk’s are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is pure. In fact, it turns out that the Gini index and the cross-entropy are quite similar numerically.
When building a classification tree, either the Gini index or the cross- entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.
Figure 8.6 shows an example on the Heart data set. These data con- tain a binary outcome HD for 303 patients who presented with chest pain. An outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease. There are 13 predic- tors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements. Cross-validation results in a tree with six terminal nodes.
In our discussion thus far, we have assumed that the predictor vari- ables take on continuous values. However, decision trees can be constructed even in the presence of qualitative predictor variables. For instance, in the Heart data, some of the predictors, such as Sex, Thal (Thalium stress test), and ChestPain, are qualitative. Therefore, a split on one of these variables amounts to assigning some of the qualitative values to one branch and
8.1 The Basics of Decision Trees 313
Thal:a
|
￼￼Ca < 0.5
Ca < 0.5
Thal:b ChestPain:a
No Yes
Thal:a
￼￼Chol < 244 MaxHR < 156
No
Chol < 244 No No
Sex < 0.5 No Yes
MaxHR < 145.5
No
Yes
No Yes
No
MaxHR < 161.5 RestBP < 157
ChestPain:bc
Slope < 1.5 Age < 52
Oldpeak < 1.1 RestECG < 1
Yes No No
Yes
Yes
|
Yes
￼￼Training Cross−Validation Test
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼MaxHR < 161.5
No No
ChestPain:bc
No Yes
Yes
Yes
Ca < 0.5
Ca < 0.5
￼￼￼￼￼￼5 10 15 Tree Size
FIGURE 8.6. Heart data. Top: The unpruned tree. Bottom Left: Cross -validation error, training, and test error, for different sizes of the pruned tree. Bottom Right: The pruned tree corresponding to the minimal cross-validation error.
assigning the remaining to the other branch. In Figure 8.6, some of the in- ternal nodes correspond to splitting qualitative variables. For instance, the top internal node corresponds to splitting Thal. The text Thal:a indicates that the left-hand branch coming out of that node consists of observations with the first value of the Thal variable (normal), and the right-hand node consists of the remaining observations (fixed or reversible defects). The text ChestPain:bc two splits down the tree on the left indicates that the left-hand branch coming out of that node consists of observations with the second and third values of the ChestPain variable, where the possible values are typical angina, atypical angina, non-anginal pain, and asymptomatic.
Error
0.0 0.1 0.2 0.3 0.4 0.5 0.6
314 8. Tree-Based Methods
Figure 8.6 has a surprising characteristic: some of the splits yield two terminal nodes that have the same predicted value. For instance, consider the split RestECG<1 near the bottom right of the unpruned tree. Regardless of the value of RestECG, a response value of Yes is predicted for those ob- servations. Why, then, is the split performed at all? The split is performed because it leads to increased node purity. That is, all 9 of the observations corresponding to the right-hand leaf have a response value of Yes, whereas 7/11 of those corresponding to the left-hand leaf have a response value of Yes. Why is node purity important? Suppose that we have a test obser- vation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is Yes. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its re- sponse value is probably Yes, but we are much less certain. Even though the split RestECG<1 does not reduce the classification error, it improves the Gini index and the cross-entropy, which are more sensitive to node purity.
8.1.3 Trees Versus Linear Models
Regression and classification trees have a very different flavor from the more classical approaches for regression and classification presented in Chapters 3 and 4. In particular, linear regression assumes a model of the form
􏰏p
f(X) = β0 +
whereas regression trees assume a model of the form
􏰏M m=1
(8.8)
(8.9)
f(X) =
cm · 1(X∈Rm)
whereR1,...,RM representapartitionoffeaturespace,asinFigure8.3. Which model is better? It depends on the problem at hand. If the relationship between the features and the response is well approximated by a linear model as in (8.8), then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure. If instead there is a highly non-linear and complex relationship between the features and the response as indicated by model (8.9), then decision trees may outperform classical approaches. An illustrative example is displayed in Figure 8.7. The rela- tive performances of tree-based and classical approaches can be assessed by estimating the test error, using either cross-validation or the validation set
approach (Chapter 5).
Of course, other considerations beyond simply test error may come into
play in selecting a statistical learning method; for instance, in certain set- tings, prediction using a tree may be preferred for the sake of interpretabil- ity and visualization.
j=1
Xjβj,
8.1 The Basics of Decision Trees 315
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼−2 −1 0 1 2
X1
−2 −1 0 1 2
X1
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼−2 −1 0 1 2
X1
−2 −1 0 1 2
X1
FIGURE 8.7. Top Row: A two-dimensional classification example in which the true decision boundary is linear, and is indicated by the shaded regions. A classical approach that assumes a linear boundary (left) will outperform a de- cision tree that performs splits parallel to the axes (right). Bottom Row: Here the true decision boundary is non-linear. Here a linear model is unable to capture the true decision boundary (left), whereas a decision tree is successful (right).
8.1.4 Advantages and Disadvantages of Trees
Decision trees for regression and classification have a number of advantages over the more classical approaches seen in Chapters 3 and 4:
▲ Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!
▲ Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.
▲ Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).
▲ Trees can easily handle qualitative predictors without the need to create dummy variables.
X2
X2
−2 −1 0 1 2
−2 −1 0 1 2
X2
X2
−2 −1 0 1 2
−2 −1 0 1 2
316 8. Tree-Based Methods
▼ Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.
However, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved. We introduce these concepts in the next section.
8.2 Bagging, Random Forests, Boosting
Bagging, random forests, and boosting use trees as building blocks to construct more powerful prediction models.
8.2.1 Bagging
The bootstrap, introduced in Chapter 5, is an extremely powerful idea. It is used in many situations in which it is hard or even impossible to directly compute the standard deviation of a quantity of interest. We see here that the bootstrap can be used in a completely different context, in order to improve statistical learning methods such as decision trees.
The decision trees discussed in Section 8.1 suffer from high variance. This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. In contrast, a procedure with low variance will yield similar results if applied repeatedly to distinct data sets; linear regression tends to have low variance, if the ratio of n to p is moderately large. Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.
Recall that given a set of n independent observations Z1,...,Zn, each with variance σ2, the variance of the mean Z ̄ of the observations is given by σ2/n. In other words, averaging a set of observations reduces variance. Hence a natural way to reduce the variance and hence increase the predic- tion accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. In other words, we could cal- culate fˆ1(x),fˆ2(x),...,fˆB(x) using B separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by
b=1
bagging
1 􏰏B ˆˆ
favg(x) = B
fb(x).
￼Of course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated
8.2 Bagging, Random Forests, Boosting 317
samples from the (single) training data set. In this approach we generate B different bootstrapped training data sets. We then train our method on the bth bootstrapped training set in order to get fˆ∗b(x), and finally average all the predictions, to obtain
fˆ (x) = bag
1 􏰏B B b=1
fˆ∗b(x).
￼This is called bagging.
While bagging can improve predictions for many regression methods,
it is particularly useful for decision trees. To apply bagging to regression trees, we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.
Thus far, we have described the bagging procedure in the regression context, to predict a quantitative outcome Y . How can bagging be extended to a classification problem where Y is qualitative? In that situation, there are a few possible approaches, but the simplest is as follows. For a given test observation, we can record the class predicted by each of the B trees, and take a majority vote: the overall prediction is the most commonly occurring class among the B predictions.
Figure 8.8 shows the results from bagging trees on the Heart data. The test error rate is shown as a function of B, the number of trees constructed using bootstrapped training data sets. We see that the bagging test error rate is slightly lower in this case than the test error rate obtained from a single tree. The number of trees B is not a critical parameter with bagging; using a very large value of B will not lead to overfitting. In practice we use a value of B sufficiently large that the error has settled down. Using B = 100 is sufficient to achieve good performance in this example.
Out-of-Bag Error Estimation
It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations.3 The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in
3This relates to Exercise 2 of Chapter 5.
majority vote
out-of-bag
￼
318 8. Tree-Based Methods
￼￼￼￼￼Test: Bagging
Test: RandomForest OOB: Bagging
OOB: RandomForest
￼￼￼￼￼￼￼￼0 50 100 150 200 250 300
Number of Trees
FIGURE 8.8. Bagging and random forest results for the Heart data. The test error (black and orange) is shown as a function of B, the number of bootstrapped training sets used. Random forests were applied with m = √p. The dashed line indicates the test error resulting from a single classification tree. The green and blue traces show the OOB error, which in this case is considerably lower.
which that observation was OOB. This will yield around B/3 predictions for the ith observation. In order to obtain a single prediction for the ith observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal). This leads to a single OOB prediction for the ith observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. Figure 8.8 displays the OOB error on the Heart data. It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.
Variable Importance Measures
As we have discussed, bagging typically results in improved accuracy over prediction using a single tree. Unfortunately, however, it can be difficult to interpret the resulting model. Recall that one of the advantages of decision
￼Error
0.10 0.15 0.20 0.25 0.30
Fbs RestECG ExAng Sex Slope Chol Age RestBP MaxHR Oldpeak ChestPain Ca Thal
8.2 Bagging, Random Forests, Boosting 319
￼￼￼￼￼￼￼￼￼￼￼￼￼￼0 20 40 60 80 100 Variable Importance
FIGURE 8.9. A variable importance plot for the Heart data. Variable impor- tance is computed using the mean decrease in Gini index, and expressed relative to the maximum.
trees is the attractive and easily interpreted diagram that results, such as the one displayed in Figure 8.1. However, when we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure. Thus, bagging improves prediction accuracy at the expense of interpretability.
Although the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS (8.1) is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees.
A graphical representation of the variable importances in the Heart data isshowninFigure8.9.WeseethemeandecreaseinGiniindexforeachvari- able, relative to the largest. The variables with the largest mean decrease in Gini index are Thal, Ca, and ChestPain.
variable importance
320 8. Tree-Based Methods
8.2.2 Random Forests
Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. A fresh sample of m predictors is taken at each split, and typically we choose m ≈ √p—that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data).
In other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a num- ber of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Un- fortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quanti- ties. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.
Random forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average (p − m)/p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.
The main difference between bagging and random forests is the choice of predictor subset size m. For instance, if a random forest is built using m = p, then this amounts simply to bagging. On the Heart data, random forests using m = √p leads to a reduction in both test error and OOB error over bagging (Figure 8.8).
Using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors. We applied random forests to a high-dimensional biological data set consisting of ex- pression measurements of 4,718 genes measured on tissue samples from 349 patients. There are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions. In this data set, each of the patient samples has a qualitative label with 15 different levels: either normal or 1 of 14 different types of cancer. Our goal was to use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set.
random forest
￼￼
8.2 Bagging, Random Forests, Boosting 321
￼￼￼m=p m=p/2 m= p
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0 100 200 300 400 500
Number of Trees
FIGURE 8.10. Results from random forests for the 15-class gene expression data set with p = 500 predictors. The test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of m, the number of predictors available for splitting at each interior tree node. Random forests (m < p) lead to a slight improvement over bagging (m = p). A single classification tree has an error rate of 45.7 %.
We randomly divided the observations into a training and a test set, and applied random forests to the training set for three different values of the number of splitting variables m. The results are shown in Figure 8.10. The error rate of a single tree is 45.7 %, and the null rate is 75.4 %.4 We see that using 400 trees is sufficient to give good performance, and that the choice m = √p gave a small improvement in test error over bagging (m = p) in this example. As with bagging, random forests will not overfit if we increase B, so in practice we use a value of B sufficiently large for the error rate to have settled down.
8.2.3 Boosting
We now discuss boosting, yet another approach for improving the predic- tions resulting from a decision tree. Like bagging, boosting is a general approach that can be applied to many statistical learning methods for re- gression or classification. Here we restrict our discussion of boosting to the context of decision trees.
Recall that bagging involves creating multiple copies of the original train- ing data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predic- tive model. Notably, each tree is built on a bootstrap data set, independent
4The null rate results from simply classifying each observation to the dominant class overall, which is in this case the normal class.
boosting
￼￼Test Classification Error
0.2 0.3 0.4 0.5
322 8. Tree-Based Methods
of the other trees. Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.
￼Algorithm 8.2 Boosting for Regression Trees ˆ
￼1. Setf(x)=0andri =yi foralliinthetrainingset. 2. Forb=1,2,...,B,repeat:
(a) Fitatreefˆb withdsplits(d+1terminalnodes)tothetraining data (X, r).
(b) Update fˆ by adding in a shrunken version of the new tree:
ˆˆˆ f(x) ← f(x) + λfb(x).
(c) Update the residuals,
ri ←ri −λfb(xi).
3. Output the boosted model,
􏰏B ˆˆ
b=1
(8.10)
(8.11)
(8.12)
f(x) =
λfb(x).
ˆ
￼Consider first the regression setting. Like bagging, boosting involves com- bining a large number of decision trees, fˆ1, . . . , fˆB. Boosting is described in Algorithm 8.2.
What is the idea behind this procedure? Unlike fitting a single large deci- sion tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly. Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the re- sponse. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm. By fitting small trees to the residuals, we slowly improve fˆ in areas where it does not perform well. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the resid- uals. In general, statistical learning approaches that learn slowly tend to perform well. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.
8.2 Bagging, Random Forests, Boosting 323
￼￼Boosting: depth=1 Boosting: depth=2 RandomForest: m= p
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼0 1000 2000 3000 4000 5000 Number of Trees
FIGURE 8.11. Results from performing boosting and random forests on the 15-class gene expression data set in order to predict cancer versus normal. The test error is displayed as a function of the number of trees. For the two boosted models, λ = 0.01. Depth-1 trees slightly outperform depth-2 trees, and both out- perform the random forest, although the standard errors are around 0.02, making none of these differences significant. The test error rate for a single tree is 24 %.
We have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here.
Boosting has three tuning parameters:
1. The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B.
2. Theshrinkageparameterλ,asmallpositivenumber.Thiscontrolsthe rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.
3. The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables.
In Figure 8.11, we applied boosting to the 15-class cancer gene expression data set, in order to develop a classifier that can distinguish the normal class from the 14 cancer classes. We display the test error as a function of the total number of trees and the interaction depth d. We see that simple
stump
interaction depth
Test Classification Error
0.05 0.10 0.15 0.20 0.25
324 8. Tree-Based Methods
stumps with an interaction depth of one perform well if enough of them are included. This model outperforms the depth-two model, and both out- perform a random forest. This highlights one difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient. Using smaller trees can aid in interpretability as well; for instance, using stumps leads to an additive model.
8.3 Lab: Decision Trees
8.3.1 Fitting Classification Trees
The tree library is used to construct classification and regression trees. > library(tree)
We first use classification trees to analyze the Carseats data set. In these data, Sales is a continuous variable, and so we begin by recoding it as a binary variable. We use the ifelse() function to create a variable, called High, which takes on a value of Yes if the Sales variable exceeds 8, and takes on a value of No otherwise.
> library(ISLR)
> attach(Carseats)
> High=ifelse(Sales <=8,"No","Yes")
Finally, we use the data.frame() function to merge High with the rest of the Carseats data.
> Carseats =data.frame(Carseats ,High)
Wenowusethetree()functiontofitaclassificationtreeinordertopredict High using all variables but Sales. The syntax of the tree() function is quite similar to that of the lm() function.
> tree.carseats =tree(High∼.-Sales ,Carseats )
The summary() function lists the variables that are used as internal nodes
in the tree, the number of terminal nodes, and the (training) error rate.
> summary(tree.carseats)
ifelse()
￼￼￼￼￼￼￼￼￼￼￼tree()
￼￼￼￼￼Classification tree:
tree(formula = High ∼ . - Sales , data = Carseats ) Variables actually used in tree construction:
[1] "ShelveLoc " "Price" "Income"
[5] "Population " "Advertising " "Age"
Number of terminal nodes: 27
Residual mean deviance: 0.4575 = 170.7 / 373 Misclassification error rate: 0.09 = 36 / 400
￼￼￼"CompPrice " "US"
￼￼￼￼
We see that the training error rate is 9 %. For classification trees, the de- viance reported in the output of summary() is given by
−2􏰏􏰏nmk logpˆmk, mk
where nmk is the number of observations in the mth terminal node that belong to the kth class. A small deviance indicates a tree that provides a good fit to the (training) data. The residual mean deviance reported is simply the deviance divided by n−|T0|, which in this case is 400−27 = 373.
One of the most attractive properties of trees is that they can be graphically displayed. We use the plot() function to display the tree struc- ture, and the text() function to display the node labels. The argument pretty=0 instructs R to include the category names for any qualitative pre- dictors, rather than simply displaying a letter for each category.
> plot(tree.carseats )
> text(tree.carseats ,pretty =0)
The most important indicator of Sales appears to be shelving location, since the first branch differentiates Good locations from Bad and Medium locations.
If we just type the name of the tree object, R prints output corresponding to each branch of the tree. R displays the split criterion (e.g. Price<92.5), the number of observations in that branch, the deviance, the overall prediction for the branch (Yes or No), and the fraction of observations in that branch that take on values of Yes and No. Branches that lead to terminal nodes are indicated using asterisks.
> tree.carseats
node), split, n, deviance, yval, (yprob)
* denotes terminal node
1) root 400 541.5 No ( 0.590 0.410 )
2) ShelveLoc: Bad,Medium 315 390.6 No ( 0.689 0.311 ) 4) Price < 92.5 46 56.53 Yes ( 0.304 0.696 )
8) Income < 57 10 12.22 No ( 0.700 0.300 )
In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. The predict() function can be used for this purpose. In the case of a classification tree, the argument type="class" instructs R to return the actual class prediction. This approach leads to correct predictions for around 71.5 % of the locations in the test data set.
> set . seed (2)
> train=sample(1:nrow(Carseats), 200)
> Carseats .test=Carseats [-train ,]
> High.test=High[-train]
8.3 Lab: Decision Trees 325
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
326 8. Tree-Based Methods
￼￼> tree.carseats=tree(High∼.-Sales,Carseats,subset=train)
> tree.pred=predict(tree.carseats,Carseats.test,type="class")
> table(tree.pred ,High.test)
High.test tree.pred No Yes No 86 27 Yes 30 57
> (86+57) /200 [1] 0.715
Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN=prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree con- sidered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to α in (8.4)).
> set . seed (3)
> cv.carseats =cv.tree(tree.carseats ,FUN=prune.misclass )
> names(cv.carseats )
[1] "size" "dev" "k"
> cv.carseats
$size
[1]19171413 9 7 3 2 1
cv.tree()
￼￼￼￼￼￼￼￼￼￼￼￼￼"method"
￼￼￼￼￼$dev [1]
$k [1]
[8]
55 55 53 52 50 56 69 65 80
￼￼￼￼-Inf 0.0000000 2.0000000 4.2500000
5.0000000 23.0000000
0.6666667
1.0000000
1.7500000
￼￼￼￼$method
[1] "misclass "
￼￼￼attr(,"class")
[1] "prune" "tree.sequence "
￼Note that, despite the name, dev corresponds to the cross-validation error rate in this instance. The tree with 9 terminal nodes results in the lowest cross-validation error rate, with 50 cross-validation errors. We plot the error rate as a function of both size and k.
> par(mfrow=c(1,2))
> plot(cv.carseats$size ,cv.carseats$dev ,type="b")
> plot(cv.carseats$k ,cv.carseats$dev ,type="b")
￼￼￼￼￼￼￼￼￼￼￼
We now apply the prune.misclass() function in order to prune the tree to prune.
obtain the nine-node tree.
> prune.carseats=prune.misclass(tree.carseats,best=9)
> plot(prune.carseats )
> text(prune.carseats,pretty=0)
How well does this pruned tree perform on the test data set? Once again, we apply the predict() function.
> tree.pred=predict(prune.carseats,Carseats.test,type="class")
> table(tree.pred ,High.test)
High.test tree.pred No Yes No 94 24 Yes 22 60
> (94+60) /200 [1] 0.77
Now 77 % of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also improved the classification accuracy.
If we increase the value of best, we obtain a larger pruned tree with lower classification accuracy:
> prune.carseats=prune.misclass(tree.carseats,best=15)
> plot(prune.carseats )
> text(prune.carseats,pretty=0)
> tree.pred=predict(prune.carseats,Carseats.test,type="class")
> table(tree.pred ,High.test)
High.test tree.pred No Yes No 86 22 Yes 30 62
> (86+62) /200 [1] 0.74
8.3.2 Fitting Regression Trees
Here we fit a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data.
> library(MASS)
> set . seed (1)
> train = sample(1:nrow(Boston), nrow(Boston)/2)
> tree.boston=tree(medv∼.,Boston ,subset=train)
> summary(tree.boston)
Regression tree:
tree(formula = medv ∼ ., data = Boston, subset = train) Variables actually used in tree construction:
[1] "lstat" "rm" "dis"
Number of terminal nodes: 8
misclass()
8.3 Lab: Decision Trees 327
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
328 8. Tree-Based Methods
￼Residual mean deviance: 12.65 = 3099 / 245 Distribution of residuals :
Min. 1st Qu. Median Mean 3rd Qu. Max. -14.1000 -2.0420 -0.0536 0.0000 1.9600 12.6000
Notice that the output of summary() indicates that only three of the vari- ables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.
> plot(tree.boston)
> text(tree.boston ,pretty=0)
The variable lstat measures the percentage of individuals with lower socioeconomic status. The tree indicates that lower values of lstat cor- respond to more expensive houses. The tree predicts a median house price of $46, 400 for larger homes in suburbs in which residents have high socioe- conomic status (rm>=7.437 and lstat<9.715).
Now we use the cv.tree() function to see whether pruning the tree will improve performance.
> cv.boston=cv.tree(tree.boston)
> plot(cv.boston$size ,cv.boston$dev ,type=’b’)
In this case, the most complex tree is selected by cross-validation. How- ever, if we wish to prune the tree, we could do so as follows, using the prune.tree() function:
> prune.boston=prune.tree(tree.boston ,best=5)
> plot(prune.boston)
> text(prune.boston ,pretty=0)
In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.
> yhat=predict(tree.boston ,newdata=Boston[-train ,])
> boston.test=Boston[-train ,"medv"]
> plot(yhat,boston.test)
> abline (0 ,1)
> mean((yhat-boston.test)^2) [1] 25.05
In other words, the test set MSE associated with the regression tree is 25.05. The square root of the MSE is therefore around 5.005, indicating that this model leads to test predictions that are within around $5, 005 of the true median home value for the suburb.
8.3.3 Bagging and Random Forests
Here we apply bagging and random forests to the Boston data, using the randomForest package in R. The exact results obtained in this section may depend on the version of R and the version of the randomForest package
prune.tree()
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
installed on your computer. Recall that bagging is simply a special case of
a random forest with m = p. Therefore, the randomForest() function can random be used to perform both random forests and bagging. We perform bagging Forest() as follows:
> library(randomForest)
> set . seed (1)
> bag.boston=randomForest(medv∼.,data=Boston,subset=train,
mtry=13,importance =TRUE)
> bag . boston
Call:
randomForest(formula = medv ∼ ., data = Boston, mtry = 13,
importance = TRUE, subset = train)
Type of random forest : regression
Number of trees: 500 No. of variables tried at each split: 13
Mean of squared residuals : 10.77 % Var explained : 86.96
The argument mtry=13 indicates that all 13 predictors should be considered for each split of the tree—in other words, that bagging should be done. How well does this bagged model perform on the test set?
> yhat.bag = predict(bag.boston ,newdata=Boston[-train ,])
> plot(yhat.bag, boston.test)
> abline (0 ,1)
> mean((yhat.bag-boston.test)^2)
[1] 13.16
The test set MSE associated with the bagged regression tree is 13.16, almost half that obtained using an optimally-pruned single tree. We could change the number of trees grown by randomForest() using the ntree argument:
> bag.boston=randomForest(medv∼.,data=Boston,subset=train, mtry=13,ntree=25)
> yhat.bag = predict(bag.boston ,newdata=Boston[-train ,])
> mean((yhat.bag-boston.test)^2) [1] 13.31
Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses p/3 variables when building a random forest of regression trees, and √p variables when building a random forest of classification trees. Here we use mtry = 6.
> set . seed (1)
> rf.boston=randomForest(medv∼.,data=Boston,subset=train,
mtry=6,importance =TRUE)
> yhat.rf = predict(rf.boston ,newdata=Boston[-train ,])
> mean((yhat.rf-boston.test)^2) [1] 11.31
8.3 Lab: Decision Trees 329
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
330 8. Tree-Based Methods
The test set MSE is 11.31; this indicates that random forests yielded an improvement over bagging in this case.
Using the importance() function, we can view the importance of each importance() variable.
￼￼> importance (rf.boston)
￼￼crim
zn indus chas nox
rm
age
dis
rad
tax ptratio black lstat
%IncMSE 12.384 2.103 8.390 2.294 12.791 30.754 10.334 14.641 3.583 8.139 11.274 8.097 30.962
IncNodePurity 1051.54 50.31 1017.64 56.32 1107.31 5917.26 552.27 1223.93 84.30 435.71 817.33 367.00 7713.63
￼￼￼￼￼￼￼￼￼￼￼￼Two measures of variable importance are reported. The former is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The latter is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees (this was plotted in Figure 8.9). In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can be produced using the varImpPlot() function.
> varImpPlot (rf.boston)
The results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variables.
8.3.4 Boosting
Hereweusethegbmpackage,andwithinitthegbm()function,tofitboosted regression trees to the Boston data set. We run gbm() with the option distribution="gaussian" since this is a regression problem; if it were a bi- nary classification problem, we would use distribution="bernoulli". The argument n.trees=5000 indicates that we want 5000 trees, and the option interaction.depth=4 limits the depth of each tree.
> library(gbm)
> set . seed (1)
> boost.boston=gbm(medv∼.,data=Boston[train,],distribution=
"gaussian ",n.trees=5000, interaction .depth=4)
The summary() function produces a relative influence plot and also outputs the relative influence statistics.
varImpPlot()
gbm()
￼￼￼￼￼￼￼￼
￼>
var lstat rm dis crim nox ptratio black age tax 10 indus 11 chas 12 rad 13 zn
8.3 Lab: Decision Trees 331
￼￼1 2 3 4 5 6 7 8 9
rel . inf 45.96 31.22
6.81 4.07 2.56 2.27 1.80 1.64 1.36 1.27 0.80 0.20 0.015
summary(boost.boston)
￼￼￼￼￼￼￼￼￼￼￼￼We see that lstat and rm are by far the most important variables. We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.
partial dependence plot
￼￼￼￼￼￼￼￼> > >
>
par(mfrow=c(1,2)) plot(boost.boston ,i="rm") plot(boost.boston ,i="lstat")
We now use the boosted model to predict medv on the test set:
yhat.boost=predict(boost.boston,newdata=Boston[-train,], n.trees=5000)
￼￼￼￼￼￼>
[1] 11.8
mean((yhat.boost -boston.test)^2)
￼The test MSE obtained is 11.8; similar to the test MSE for random forests and superior to that for bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter λ in (8.10). The default value is 0.001, but this is easily modified. Here we take λ = 0.2.
> boost.boston=gbm(medv∼.,data=Boston[train,],distribution= "gaussian ",n.trees=5000, interaction .depth=4,shrinkage =0.2, verbose =F)
> yhat.boost=predict(boost.boston,newdata=Boston[-train,], n.trees=5000)
> mean((yhat.boost -boston.test)^2) [1] 11.5
In this case, using λ = 0.2 leads to a slightly lower test MSE than λ = 0.001.
￼￼￼￼￼￼￼￼
332 8. Tree-Based Methods
8.4 Exercises
Conceptual
1. Draw an example (of your own invention) of a partition of two- dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all as- pects of your figures, including the regions R1, R2, . . ., the cutpoints t1,t2,..., and so forth.
Hint: Your result should look something like Figures 8.1 and 8.2.
2. It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form
3. Consider the Gini index, classification error, and cross-entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of pˆm1. The x- axis should display pˆm1, ranging from 0 to 1, and the y-axis should display the value of the Gini index, classification error, and entropy.
Hint: In a setting with two classes, pˆm1 = 1 − pˆm2. You could make this plot by hand, but it will be much easier to make in R.
4. This question relates to the plots in Figure 8.12.
(a) Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of Figure 8.12. The num- bers inside the boxes indicate the mean of Y within each region.
(b) Create a diagram similar to the left-hand panel of Figure 8.12, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region.
5. Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X):
0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.
􏰏p j=1
f(X) =
Explain why this is the case. You can begin with (8.12) in
Algorithm 8.2.
fj(Xj).
X2 1 0
01 X1
X1 < 0
2.49
−1.06 0.21 FIGURE 8.12. Left: A partition of the predictor space corresponding to Exer-
−1.80
cise 4a. Right: A tree corresponding to Exercise 4b.
There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches?
6. Provide a detailed explanation of the algorithm that is used to fit a regression tree.
Applied
7. In the lab, we applied random forests to the Boston data using mtry=6 and using ntree=25 and ntree=500. Create a plot displaying the test error resulting from random forests on this data set for a more com- prehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.
8. In the lab, a classification tree was applied to the Carseats data set af- ter converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.
(a) Split the data set into a training set and a test set.
(b) Fit a regression tree to the training set. Plot the tree, and inter- pret the results. What test MSE do you obtain?
(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to de- termine which variables are most important.
X1 < 1
X2 < 2
0.63
8.4 Exercises 333
X2 < 1
￼15
￼￼5
￼3
0
￼10
￼￼
334
8. Tree-Based Methods
(e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which vari- ables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.
9. This problem involves the OJ data set which is part of the ISLR package.
(a) Create a training set containing a random sample of 800 obser- vations, and a test set containing the remaining observations.
(b) Fit a tree to the training data, with Purchase as the response and the other variables except for Buy as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
(c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
(d) Create a plot of the tree, and interpret the results.
(e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
(f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.
(g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
(h) Which tree size corresponds to the lowest cross-validated classi- fication error rate?
(i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
(j) Compare the training error rates between the pruned and un- pruned trees. Which is higher?
(k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
10. We now use boosting to predict Salary in the Hitters data set.
(a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries.
(b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.
(c) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter λ. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.
(d) Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.
(e) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.
(f) Which variables appear to be the most important predictors in the boosted model?
(g) Now apply bagging to the training set. What is the test set MSE for this approach?
11. This question uses the Caravan data set.
(a) Create a training set consisting of the first 1,000 observations,
and a test set consisting of the remaining observations.
(b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
(c) Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated prob- ability of purchase is greater than 20 %. Form a confusion ma- trix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?
12. Apply boosting, bagging, and random forests to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?
8.4 Exercises 335
